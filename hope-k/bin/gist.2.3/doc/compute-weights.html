 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
FILE: compute-weights.html
AUTHOR: William Stafford Noble
CREATE DATE: 7/15/99
PROJECT: SVM
REVISION: $Revision: 1.2 $
-->
<HEAD>
<TITLE>gist-train-svm</TITLE>
</head>
<BODY BGCOLOR=white>
<CENTER><H1>gist-train-svm</H1></CENTER>

<BLOCKQUOTE>

<P><B>Description:</B> Train a support vector machine using a simple
iterative update procedure first described by Jaakkola, Diekhans and
Haussler.

<P><B>Usage:</B> <CODE>gist-train-svm [options]
-train&nbsp;&lt;train&nbsp;filename&gt;
-class&nbsp;&lt;class&nbsp;filename&gt;</CODE>

<P><B>Input:</B> 
<ul> 

<li> &lt;train filename&gt; - a tab-delimited file of training
examples.  The first column contains labels, and the remaining columns
contain real-valued features.  Missing values are not allowed.

<li> &lt;class filename&gt; - a multi-column, tab-delimited file of
training set labels.  This file must contain exactly the same number
of lines as the training data file.  The first column contains labels,
which must appear in the same order as in the training data file.  The
second and subsequent columns contain binary classifications (1 for
positive examples, -1 or 0 for negatives). The classification column
used from this file is the first one by default; subsequent columns
can be used by invoking the -useclassnumber option described below.
</ul>

<P><B>Output:</B> A five-column, tab-delimited file.  The first two
columns are identical to the classification file that was provided as
input.  Column three contains learned weights for the SVM, each
multiplied by the corresponding label.  Columns four and five contain
the predicted classification and the corresponding discriminant value.
This output file is suitable for input to <A
HREF="classify.html">classify</A>.</P>

<P><B>Options:</B>
<UL>

<li> -useclassnumber &lt;value&gt; - If the class file contains multiple
classes, use the class indicated by this number. The first column of
class labels is column 1. If this option is omitted, the first column
of classifications is used.

<li> -initial &lt;file&gt; - Initialize the weights to the given
values.  The weights should appear in column 3 of the file.  Output
files produced by this program may be used to initialize the weights.

<li> -hyperplane &lt;file&gt; - Print to the given file the
coordinates of the hyperplane.  This option is only allowed when the
kernel is linear; i.e., when the <code>-radial</code>,
<code>-power</code>, <code>-coefficient</code> and
<code>-matrix</code> options are not specified.  The resulting file
can be used as input to the <code>gist-fast-classify</code>
program.</li>

<li> -holdout &lt;percent&gt; - Add two additional columns to the
output, which will contain the predicted classification and
corresponding discriminant values computed via hold-one-out
cross-validation.  The specified &lt;percent&gt; determines what
percentage of the training set will be randomly selected for
hold-one-out cross-validation.  For the remaining, non-held-out
examples, the final two columns will contain the value "NaN".
Note that the weights reported using this switch are the weights
obtained from training on the entire data set.

<li> -zeromeanrow - Subtract from each element in the input data the
mean of the elements in that row, giving the row a mean of zero.

<li> -varone - Divide each element in the input data by the standard
deviation of the elements in that row, giving the row a variance of
one.


<li> -matrix - By default, the base kernel function is a dot product.
This option allows that function to be replaced by an arbitrary
function supplied by the user (for many commonly used kernels, see the
options listed below).  If supplied, the software reads kernel values,
rather than raw feature data, from the file specified by
<code>-train</code>.  The matrix must be an n+1 by n+1 tab-delimited
matrix, where n is the number of training examples.  The first row and
column contain data labels.  The matrix entry for row x, column y,
contains the kernel value K(x,y). <strong>Note</strong> that if the
kernel matrix was generated by gist-train-svm (and then possibly
modified), then you should also set "<code>-constant 0
-nonormalize</code>" when using -matrix, to avoid these
transformations being applied to the kernel matrix a second time.

    <strong>Further Note</strong> that there are
special options that must be invoked during later classification using
<a href="classify.html">classify</a> with an SVM trained using the
-matrix option. These include the -selftrain and -selftest
options. See the documentation for <a
href="classify.html">classify</a> for details.</li>

</ul>

<p>The following four options control feature selection, which is only
available in conjunction with hold-one-out cross-validation.  In order
to perform feature selection on distinct training and test sets, you
must first use <A HREF="fselect.html">fselect</A> to select a feature
subset.</p>

<ul>

<li>
-fselect fisher|ttest|welch|mannwhitney|sam|tnom - Specify the
metric used to evaluate individual features.  See the documentation
for <A HREF="fselect.html">fselect</A> for more information.

<li>-fthreshtype percent|number|value - Select different means of setting
the feature selection threshold.  The "percent" option chooses the top
n% of the features.  The "number" option chooses the top n features.
The "value" option chooses features that score above n.  The default
setting is "percent".</li>

<li> -fthreshold &lt;value&gt; - Set the threshold for feature
selection.  The default setting depends upon the threshold type: for
"percent" and "number", the default is 10; for "value" it is 1. This
threshold is not to be confused with the SVM stopping criterion, which
is set by -threshold.</li>

<li>
-fscores &lt;file&gt; - Write to the given file a matrix containing
the computed quality scores for each feature.  Each row corresponds to
one feature.  The first column contains the feature name, and the
second column contains the the Fisher score, the t-test score, or the
negative log<SUB>2</SUB> of the t-test p-value.  If the "-holdout"
option is specified, additional columns are included, corresponding to
each held-out example.

</UL>
The following eight options modify the base kernel function.  The
operations occur in the order listed below.
<UL>

<li> -adddiag &lt;value&gt; - Add the given value to the diagonal of
the training kernel matrix.  This option effects a 2-norm soft margin
and should therefore not be used in conjunction with the
<CODE>-posconstraint</CODE> and <CODE>-negconstraint</CODE> options.
The default value is 0.

<li> -nonormalize - Do not normalize the kernel matrix.  By default,
the matrix is normalized by dividing K(x,y) by sqrt(K(x,x) * K(y,y)).

<li> -constant &lt;value&gt; - Add a given constant to the kernel.
The default constant is 10.

<li> -coefficient &lt;value&gt; - Multiply the kernel by a given
coefficient.  The default coefficient is 1.

<li> -power &lt;value&gt; - Raise the kernel to a given power.  The
default power is 1.

<li> -radial - Convert the kernel to a radial basis function.  If K is
the base kernel, this option creates a kernel of the form
exp[(-D(x,y)<SUP>2</SUP>)/(2 w<SUP>2</SUP>)], where w is the width of
the kernel (see below) and D(x,y) is the distance between x and y,
defined as D(x,y) = sqrt[K(x,x) - 2 K(x,y) + K(y,y)].

<li> -widthfactor &lt;value&gt; - The width w of the radial basis
kernel is set using a heuristic: it is the median of the distance from
each positive training point to the nearest negative training point.
This option specifies a multiplicative factor to be applied to that
width.  The default is a width factor of 1.</LI>

<LI> -width &lt;value&gt; - Directly set the width w of the radial
basis kernel.  If set, this option overrides the -widthfactor option.

<li> -diagfactor &lt;value&gt; - Add to the diagonal of the kernel
matrix (n<SUP>+</SUP>/N) * m * k, where n<SUP>+</SUP> is the number of
positive training examples if the current example is positive (and
similarly for negative training examples), N is the total number of
training examples, m is the median value of the diagonal of the kernel
matrix, and k is the value specified here.  This option effects a
2-norm soft margin and should therefore not be used in conjunction
with the <CODE>-posconstraint</CODE> and <CODE>-negconstraint</CODE>
options.  The default diagonal factor is 0.1. Note that the diagonal
factor is not applied if the <code>-kernelout</code> option is set.

<li> -posconstraint &lt;value&gt; - Set an explicit upper bound on the
magnitude of the weights for positive training examples.  By default,
the magnitude is unconstrained.  Note that this option (and the next)
should be used in combination with a <CODE>-diagfactor</CODE> of 0.

<li> -negconstraint &lt;value&gt; - Set an explicit upper bound on the
magnitude of the weights for negative training examples.  By default,
the magnitude is unconstrained.

<li> -rdb - Allow the program to read and create <A
HREF="http://www.cse.ucsc.edu/research/compbio/rdb/index.html">RDB</A>
formatted files, which contain an additional format line after the
first line of text.

<li> -kernelout - Compute and print the kernel matrix to stdout.  Do
not compute the weights and do not add to the kernel diagaonal.</li>

<li> -threshold &lt;value&gt; - Set the convergence threshold.
Training halts when the objective function changes by less than this
amount.  Default is 0.000001.  Note that lowering the threshold also
increases the precision with which weights are reported by the
program.  Note that this threshold has nothing to do with feature
selection: the feature seleciton threshold is set with
-fthreshold.</li>

<li> -maxiter &lt;value&gt; - Set the maximum number of iterations for
the optimization routine.  Default is no limit.  If -maxiter and
-maxtime are both set, the optimization will stop when either limit is
reached.  If a limit is reached, the program quits without producing
any output.

<li> -maxtime &lt;seconds&gt; - Set the maximum time in seconds for
the optimization routine.  This limit applies only to a single SVM
optimization, not to the total running time of hold-one-out
cross-validation.  Default is no limit.  If -maxiter and -maxtime are
both set, the optimization will stop when either limit is reached.  If
a limit is reached, the program quits without producing any output.

<li> -seed &lt;value&gt; - Set the seed for the random number
generator.  By default the seed is set from the clock.

<li> -notime - Do not include timing information in the output header.

<li> -precision &lt;value&gt; - Number of digits after the decimal
place in the output file.  By default, this value is set equal to the
maximum of 4 and the log of the convergence threshold minus 2.

<li> -verbose 1|2|3|4|5 - Set the verbosity level of the output to
stderr.  The default level is 2.
</UL>

<P>
<B>Warning messages:</B>

<UL>

<li>
<P>
<I>Warning: Zero-valued diagonal kernel element:row 0 = 2.99937, col
163 = 0.</I></P>

<P>
This warning is issued when normalization of the kernel causes a
divide-by-zero error.  The message above indicates that the
(unnormalized) 163rd diagonal kernel matrix entry (i.e., at row 163
and column 163) has a value of zero.  Therefore, when normalizing the
kernel value in row 0, column 163, Gist attempts to divide by zero,
which is not possible.  Gist circumvents the problem by dividing
instead by a very small number (0.0000000001), but the warning is
issued to let you know about the problem.  Note that in order to avoid
producing too many warnings, the warning is issued only once, even
though the divide-by-zero happens many times (in this case, it will
also happen in row 1, 2, etc.).  Typically, this error is caused by a
row of all zeroes in the input data.</P>

<li>
<P>
<I>Warning: possible underflow in radial_kernel</I></P>

<P>
This error indicates that the exponentation carried out by the radial
basis function produced a value that is smaller than can be
represented by the computer.  This will occur when your data contains
many features and many large values.  A possible solution is to scale
your data, using for example a small <CODE>-coefficient</CODE> value.
Note that the warning is issued only once, even if the underflow
occurs many times.</P>

<li>
<P>
<I>Warning: Terminating after failure to converge in 10000
iterations.<BR>
Warning: Terminating after failure to converge in 10000
seconds.</I></P>

<P>
These warnings are issued if the computation time exceeds the limits
imposed by the <CODE>-maxiter</CODE> and <CODE>-maxtime</CODE>
options.</P>


<li>
<P>
<I>Warning: Using 1-norm and 2-norm soft margins
simultaneously.</I></P>

<P>
This warning is issued when the soft margin is specified in two ways,
both by constraining the magnitude of the weights (using
<CODE>-posconstraint</CODE> and <CODE>-negconstraint</CODE>) and by
adding to the diagonal of the kernel matrix (using
<CODE>-diagfactor</CODE>).  The optimization will still work, but it
is somewhat odd to use both types of margin at once.  Note that, by
default, the 2-norm soft margin is used.</P>

</UL>

<P><B>Bugs:</B>
<UL>
<li>
The program does not verify that the labels in the class file match
the labels in the data file.</li>
<li>
Tie breaking is not actually random in the sense that each run will
give different results, because the decision depends only on the
original order of the examples in the input files.</li>
</UL>


</BLOCKQUOTE>

<HR>
<A HREF="index.html">Gist</A>
</BODY>
</HTML>
